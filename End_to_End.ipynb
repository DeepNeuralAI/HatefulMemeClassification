{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "End_to_End.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FlLTwakVulpF",
        "d2Pz8MAhq4NU",
        "dcVWmRLzsetF",
        "MlDBlBx-tI3g",
        "Q9hei3Dosjrb",
        "A-uB7hUbAMLX",
        "P3lB9itZ9KOA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlLTwakVulpF"
      },
      "source": [
        "## GPU Info & Mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64bltJAdsECN"
      },
      "source": [
        "## GPU info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSI-EUpLsCO9",
        "outputId": "01ed55e4-9347-4dea-d46f-dfc7bebf66ce"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 29 07:28:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XfAd8lLt2ls"
      },
      "source": [
        "## Mount Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LlmOd_Vt13J"
      },
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# root_path = 'drive/My Colab Notebooks/'  #change dir to your project folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Pz8MAhq4NU"
      },
      "source": [
        "## Install & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BMULpYwCquco",
        "outputId": "910f119e-38cf-4905-bce0-088144e5babe"
      },
      "source": [
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjB7mK2aq0CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a32a1f9-9add-4c04-ec54-2953d6b567df"
      },
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu92)\n",
            "Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0+cu92)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxWcGDbnq9Y8"
      },
      "source": [
        "### Install MMF from Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOP0rwtGq_Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c80a97c-8545-422a-c14c-fb8b2b418fe0"
      },
      "source": [
        "!git clone --branch no_feats --config core.symlinks=true https://github.com/rizavelioglu/mmf.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mmf' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcwLjg0NrEJM"
      },
      "source": [
        "import os\n",
        "os.chdir(os.path.join(home, \"mmf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pux57uUQrEwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99092cb-0a7f-423a-f2d9-ed6d062b1b67"
      },
      "source": [
        "!pip install --editable ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/mmf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.0)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
            "Requirement already satisfied: GitPython==3.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.1.0)\n",
            "Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.9.1)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.4.0)\n",
            "Requirement already satisfied: omegaconf==2.0.1rc4 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.1rc4)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.6.0+cu92)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (4.61.2)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.4.5)\n",
            "Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.7.0+cu92)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Requirement already satisfied: lmdb==0.98 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.98)\n",
            "Requirement already satisfied: demjson==2.2.4 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.2.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (57.2.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.7.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython==3.1.0->mmf==1.0.0rc12) (4.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->mmf==1.0.0rc12) (0.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (0.1.96)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (0.0.45)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (0.9.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.0->mmf==1.0.0rc12) (4.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->mmf==1.0.0rc12) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
            "Installing collected packages: mmf\n",
            "  Attempting uninstall: mmf\n",
            "    Found existing installation: mmf 1.0.0rc12\n",
            "    Can't uninstall 'mmf'. No files were found to uninstall.\n",
            "  Running setup.py develop for mmf\n",
            "Successfully installed mmf-1.0.0rc12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFbz3tHIsK-H"
      },
      "source": [
        "## Convert to MMF format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jL33KQvsQ-p"
      },
      "source": [
        "zip_file_path=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/data/hateful_meme_data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuCn7PntsQSl",
        "outputId": "90ee0262-13bf-4114-cc01-69692c860ae7"
      },
      "source": [
        "!mmf_convert_hm --zip_file=$zip_file_path --password=\"\" --bypass_checksum 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 08:00:45.076813: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is /content/drive/MyDrive/Colab_Notebooks/hateful_memes/data/hateful_meme_data.zip\n",
            "Copying /content/drive/MyDrive/Colab_Notebooks/hateful_memes/data/hateful_meme_data.zip\n",
            "Unzipping /content/drive/MyDrive/Colab_Notebooks/hateful_memes/data/hateful_meme_data.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUDaUC-09ygH",
        "outputId": "76babdea-48ba-4655-87a2-b893d200cf45"
      },
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcVWmRLzsetF"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WolpcCNptAQw"
      },
      "source": [
        "### VQA Mask-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQOs9_hWshH3"
      },
      "source": [
        "import os\n",
        "os.chdir(home)\n",
        "!git clone https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnfJB2rIsjZs"
      },
      "source": [
        "!pip install ninja yacs cython matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yhHHUxhtGbO"
      },
      "source": [
        "os.chdir(os.path.join(home, \"vqa-maskrcnn-benchmark\"))\n",
        "!rm -rf build\n",
        "!python setup.py build develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlDBlBx-tI3g"
      },
      "source": [
        "### Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXXFWVSNFTD5"
      },
      "source": [
        "#### Process Misogynistic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSiPirIpFbRw"
      },
      "source": [
        "# read the .jsonl file and get the img column\n",
        "labeled_memo_samples = pd.read_json(os.path.join(home, \"Misogynistic-MEME/train_miso.jsonl\"), lines=True)['img']\n",
        "# parse the img entries and get the image names\n",
        "labeled_memo_samples = [i.split('/')[1] for i in list(labeled_memo_samples)]\n",
        "\n",
        "img_dir = os.path.join(home, f\"Misogynistic-MEME/img/\")\n",
        "for img in labeled_memo_samples:\n",
        "    os.rename(f\"{img_dir+img}\", f\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/{img}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBYTXwqOveDy"
      },
      "source": [
        "# Load labeled misoginistis\n",
        "train = pd.read_json(\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/train.jsonl\", lines=True)\n",
        "miso = pd.read_json(os.path.join(home, \"Misogynistic-MEME/train_miso.jsonl\"), lines=True)\n",
        "# Add miso data to 'train.jsonl'\n",
        "train = pd.concat([train, miso], axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HTMuetP7_pU"
      },
      "source": [
        "# Shuffle data\n",
        "train = train.sample(frac=1).reset_index(drop=True)\n",
        "# Write new jsonl file\n",
        "train_json = train.to_json(orient='records', lines=True)\n",
        "\n",
        "with open(os.path.join(home, \"train_v11.jsonl\"), \"w\", encoding='utf-8') as f:\n",
        "    f.write(train_json)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYgMbgQ7tL6y"
      },
      "source": [
        "os.chdir(os.path.join(home, \"mmf/tools/scripts/features/\"))\n",
        "out_folder = os.path.join(home, \"features/\")\n",
        "\n",
        "!python extract_features_vmb.py --config_file \"https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model_x152.yaml\" \\\n",
        "                                --model_name \"X-152\" \\\n",
        "                                --output_folder $out_folder \\\n",
        "                                --image_dir \"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/\" \\\n",
        "                                --num_features 100 \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvX2JsKa9TR"
      },
      "source": [
        "os.chdir(home)\n",
        "# !zip -r \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/features.zip\" features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hei3Dosjrb"
      },
      "source": [
        "## Fine-tuning w/ VisualBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CL05ELPBHv0M",
        "outputId": "a31a6e59-c9b0-4173-f8fb-e0b208ca16ba"
      },
      "source": [
        "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/hateful_memes/\")\n",
        "# os.getcwd()\n",
        "# # !unzip \"features.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/hateful_memes/features'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X-91C1-fMSE9",
        "outputId": "440fc341-9b55-46f0-c734-f98d63e61f23"
      },
      "source": [
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NNCzZ4DQt0D"
      },
      "source": [
        "log_dir =\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/baseline/visual_bert\"\n",
        "save_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/submissions/baseline/visual_bert/submission_1/\"\n",
        "feats_dir = \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/features/\"\n",
        "train_dir = \"hateful_memes/defaults/annotations/train.jsonl\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD1_4iduAD4t"
      },
      "source": [
        "### Run Model with Default Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znvj2L2P2GJw",
        "outputId": "ab448477-9925-4652-9443-08fddc07d063"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX8IH2GOB8ix"
      },
      "source": [
        "%tensorboard --logdir $log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUobSz2Vsmd8"
      },
      "source": [
        "\n",
        "# # Define where train.jsonl is\n",
        "# train_dir = os.path.join(home, \"train_v9.jsonl\")\n",
        "!mmf_run \\\n",
        "        config=\"projects/visual_bert/configs/hateful_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=100 \\\n",
        "        training.evaluation_interval=100 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        dataset_config.hateful_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=$save_dir \\\n",
        "        env.tensorboard_logdir=$log_dir \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-uB7hUbAMLX"
      },
      "source": [
        "### Run VisualBERT Model with Hyper-Parameter Sweep x27"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvmmChLrB7eq"
      },
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/hyperparameter_sweep/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRtKxWcMBZ8y"
      },
      "source": [
        "hyper_params_path=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/hyperparameter_sweep\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lhrzrdnAO_-"
      },
      "source": [
        "os.chdir(hyper_params_path)\n",
        "# Give rights to bash script to be executable\n",
        "!chmod +x sweep.sh\n",
        "# Define where image features are\n",
        "feats_dir = \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/features/\"\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"hateful_memes/defaults/annotations/train.jsonl\"\n",
        "\n",
        "# Start hyper-parameter search\n",
        "!python sweep.py --home $home --feats_dir $feats_dir --train $train_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ9gt7L6acvF"
      },
      "source": [
        "### Run Single Model with Modified Hyper-parameter Configs\n",
        "\n",
        "#### Early Stopping: True\n",
        "#### FP16 Precision: True\n",
        "#### Max Updates: 1000\n",
        "#### LR Ratio: 0.1\n",
        "#### Training Warmup: False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8cbHdIhwc3yU",
        "outputId": "a865587e-6dd8-4db7-931c-d133423a8952"
      },
      "source": [
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlfMIMSoc4_M"
      },
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAs7OetPc6gX"
      },
      "source": [
        "tensorboard_log_dir =\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/\"\n",
        "save_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/submissions/\"\n",
        "feats_dir = \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/features/\"\n",
        "train_dir = \"hateful_memes/defaults/annotations/train.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u0CojvLWafwj"
      },
      "source": [
        "!mmf_run \\\n",
        "        config=\"projects/visual_bert/configs/hateful_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        training.lr_ratio=0.1 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=40 \\\n",
        "        training.evaluate_metrics=True \\\n",
        "        training.early_stop.enabled=True \\\n",
        "        training.early_stop.criteria='hateful_memes/roc_auc' \\\n",
        "        training.early_stop.minimize=False \\\n",
        "        training.fp16=True \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        dataset_config.hateful_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.test[0]=$feats_dir \\\n",
        "        env.save_dir=$save_dir \\\n",
        "        env.tensorboard_logdir=$tensorboard_log_dir \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3lB9itZ9KOA"
      },
      "source": [
        "## Fine-Tuning via ViLBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fZecFcjE9Qau",
        "outputId": "6fbe349d-e75b-4424-d604-e2af417d5fe7"
      },
      "source": [
        "import os\n",
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdK1vZW9Wm3"
      },
      "source": [
        "log_dir =\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/baseline/vilbert/\"\n",
        "# save_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/submissions/baseline/vilbert/\"\n",
        "save_dir=\"/content/submissions/baseline/vilbert/\"\n",
        "feats_dir = \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/features/\"\n",
        "train_dir = \"hateful_memes/defaults/annotations/train.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgJthsclSZTD"
      },
      "source": [
        "!kill 235"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_n8O8pT9ZnJ"
      },
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/baseline/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jyg3Q9L89dN_"
      },
      "source": [
        "!mmf_run \\\n",
        "        config=\"projects/vilbert/configs/hateful_memes/from_cc.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=hateful_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=5000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.hateful_memes.max_features=100 \\\n",
        "        dataset_config.hateful_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "        dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.hateful_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=40 \\\n",
        "        training.evaluate_metrics=True \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=$save_dir \\\n",
        "        env.tensorboard_logdir=$log_dir \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUsnTQAcFC7A"
      },
      "source": [
        "## Hyper-Parameter Sweep with Best Config of VisualBERT + Dataset Expansion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tHAZqEaLDto"
      },
      "source": [
        "# !unzip '/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features_miso.zip' -d '/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4LHOwXFIBc"
      },
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/Colab_Notebooks/hateful_memes')\n",
        "# from hparams_sweep import run_sweep\n",
        "# from subprocess import call"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0YulmK_F1C2"
      },
      "source": [
        "train_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl\"\n",
        "feats_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\"\n",
        "log_dir=\"/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert\"\n",
        "save_dir=\"/content/submissions/best/visual_bert\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZrK7N-FNior"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg5l0MdCWvTd"
      },
      "source": [
        "%tensorboard --logdir $log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIHTJn3ZGR8L"
      },
      "source": [
        "hparams = {\n",
        "        \"batch_size\": [30, 40, 50],\n",
        "        \"learning_rate\": [5e-3, 5e-5, 5e-6],\n",
        "        \"lr_ratio\": [0.1, 0.3, 0.5]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeU9pWsqI1oV",
        "outputId": "4220babd-217b-4efa-ba22-3b5e1760b3bc"
      },
      "source": [
        "!OC_DISABLE_DOT_ACCESS_WARNING=1\n",
        "\n",
        "for batch_size in hparams['batch_size']:\n",
        "        for lr in hparams['learning_rate']:\n",
        "            for ratio in hparams['lr_ratio']:\n",
        "\n",
        "              log_name = f'{batch_size}_{lr}_{ratio}'\n",
        "              log_path = f'{log_dir}/{log_name}'\n",
        "              save_path = f'{save_dir}/{log_name}'\n",
        "\n",
        "              print('==================================================')\n",
        "              print(f'Current: {log_name}')\n",
        "              print('==================================================\\n')\n",
        "\n",
        "              !mmf_run \\\n",
        "                config=\"projects/visual_bert/configs/hateful_memes/from_coco.yaml\" \\\n",
        "                model=\"visual_bert\" \\\n",
        "                dataset=hateful_memes \\\n",
        "                run_type=train_val \\\n",
        "                checkpoint.max_to_keep=1 \\\n",
        "                checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "                training.tensorboard=True \\\n",
        "                training.checkpoint_interval=100 \\\n",
        "                training.evaluation_interval=100 \\\n",
        "                training.max_updates=3000 \\\n",
        "                training.log_interval=100 \\\n",
        "                training.early_stop.enabled=True \\\n",
        "                training.early_stop.criteria='hateful_memes/roc_auc' \\\n",
        "                training.early_stop.minimize=False \\\n",
        "                dataset_config.hateful_memes.max_features=100 \\\n",
        "                dataset_config.hateful_memes.annotations.train[0]=$train_dir \\\n",
        "                dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "                dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "                dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "                dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "                dataset_config.hateful_memes.features.test[0]=$feats_dir \\\n",
        "                training.lr_ratio=$ratio \\\n",
        "                training.use_warmup=True \\\n",
        "                training.batch_size=$batch_size \\\n",
        "                optimizer.params.lr=$lr \\\n",
        "                env.save_dir=$save_path\\\n",
        "                env.tensorboard_logdir=$log_path \n",
        "              \n",
        "              print('==================================================')\n",
        "              print('Deleting model files for space saving')\n",
        "              !rm -rf $save_path\n",
        "              print('==================================================\\n')\n",
        "              \n",
        "              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:55:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T13:55:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.0963, train/hateful_memes/cross_entropy/avg: 0.2355, train/total_loss: 0.0963, train/total_loss/avg: 0.2355, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 137ms, time_since_start: 01h 06m 44s 124ms, eta: 0ms\n",
            "\u001b[32m2021-07-29T13:55:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:55:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:55:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:55:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:55:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T13:56:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 1.7209, val/total_loss: 1.7209, val/hateful_memes/accuracy: 0.7185, val/hateful_memes/binary_f1: 0.5366, val/hateful_memes/roc_auc: 0.7162, num_updates: 3000, epoch: 10, iterations: 3000, max_updates: 3000, val_time: 32s 124ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.727574\n",
            "\u001b[32m2021-07-29T13:56:12 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-07-29T13:56:12 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-07-29T13:56:12 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:56:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:56:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T13:56:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T13:56:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
            "\u001b[32m2021-07-29T13:56:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
            "\u001b[32m2021-07-29T13:56:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n",
            "\u001b[32m2021-07-29T13:56:32 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 18/18 [00:11<00:00,  1.54it/s]\n",
            "\u001b[32m2021-07-29T13:56:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.8505, val/total_loss: 0.8505, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5210, val/hateful_memes/roc_auc: 0.7276\n",
            "\u001b[32m2021-07-29T13:56:44 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 07m 50s 559ms\n",
            "Deleting model files for space saving\n",
            "Current: 30_5e-05_0.5\n",
            "2021-07-29 13:56:52.767409: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/from_coco.yaml\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc.full\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 100\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.enabled to True\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.criteria to hateful_memes/roc_auc\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.minimize to False\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.max_features to 100\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.5\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 30\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-05\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/submissions/best/visual_bert/30_5e-05_0.5\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-05_0.5\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf: \u001b[0mLogging to: /content/submissions/best/visual_bert/30_5e-05_0.5/train.log\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.pretrained.cc.full', 'training.tensorboard=True', 'training.checkpoint_interval=100', 'training.evaluation_interval=100', 'training.max_updates=3000', 'training.log_interval=100', 'training.early_stop.enabled=True', 'training.early_stop.criteria=hateful_memes/roc_auc', 'training.early_stop.minimize=False', 'dataset_config.hateful_memes.max_features=100', 'dataset_config.hateful_memes.annotations.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.val[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.test[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'training.lr_ratio=0.5', 'training.use_warmup=True', 'training.batch_size=30', 'optimizer.params.lr=5e-05', 'env.save_dir=/content/submissions/best/visual_bert/30_5e-05_0.5', 'env.tensorboard_logdir=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-05_0.5'])\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf_cli.run: \u001b[0mUsing seed 59802042\n",
            "\u001b[32m2021-07-29T13:56:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2021-07-29T13:57:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2021-07-29T13:57:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-07-29T13:57:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2021-07-29T13:57:20 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:57:28 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:57:28 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:57:28 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:57:28 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2021-07-29T13:57:28 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2021-07-29T13:58:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:58:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:58:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T13:59:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6690, train/total_loss: 0.6690, train/total_loss/avg: 0.6690, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 094ms, time_since_start: 01m 45s 070ms, eta: 46m 55s 747ms\n",
            "\u001b[32m2021-07-29T13:59:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:59:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:59:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:59:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T13:59:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T13:59:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/hateful_memes/cross_entropy: 0.6595, val/total_loss: 0.6595, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5045, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 50s 135ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.504485\n",
            "\u001b[32m2021-07-29T14:01:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:01:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.5390, train/hateful_memes/cross_entropy/avg: 0.6040, train/total_loss: 0.5390, train/total_loss/avg: 0.6040, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00001, ups: 1.06, time: 01m 34s 689ms, time_since_start: 04m 09s 925ms, eta: 44m 11s 309ms\n",
            "\u001b[32m2021-07-29T14:01:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:01:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:02:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/hateful_memes/cross_entropy: 0.6615, val/total_loss: 0.6615, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0566, val/hateful_memes/roc_auc: 0.5614, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 49s 597ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.561368\n",
            "\u001b[32m2021-07-29T14:03:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:03:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:03:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:03:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:03:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:03:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6413, train/total_loss: 0.6690, train/total_loss/avg: 0.6413, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00001, ups: 1.06, time: 01m 34s 673ms, time_since_start: 06m 34s 199ms, eta: 42m 36s 184ms\n",
            "\u001b[32m2021-07-29T14:03:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:04:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:04:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:04:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:04:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:04:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/hateful_memes/cross_entropy: 0.7341, val/total_loss: 0.7341, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.2463, val/hateful_memes/roc_auc: 0.5582, num_updates: 300, epoch: 1, iterations: 300, max_updates: 3000, val_time: 32s 130ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.561368\n",
            "\u001b[32m2021-07-29T14:05:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:05:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:05:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:05:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:05:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6374, train/total_loss: 0.6258, train/total_loss/avg: 0.6374, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00001, ups: 1.04, time: 01m 36s 478ms, time_since_start: 08m 42s 809ms, eta: 41m 48s 434ms\n",
            "\u001b[32m2021-07-29T14:06:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:06:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/hateful_memes/cross_entropy: 0.6946, val/total_loss: 0.6946, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.4582, val/hateful_memes/roc_auc: 0.6408, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 48s 867ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.640765\n",
            "\u001b[32m2021-07-29T14:08:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:08:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.5829, train/total_loss: 0.6258, train/total_loss/avg: 0.5829, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00001, ups: 1.08, time: 01m 33s 617ms, time_since_start: 11m 05s 297ms, eta: 39m 438ms\n",
            "\u001b[32m2021-07-29T14:08:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:08:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:09:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.7345, val/total_loss: 0.7345, val/hateful_memes/accuracy: 0.6519, val/hateful_memes/binary_f1: 0.3974, val/hateful_memes/roc_auc: 0.6467, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 50s 293ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.646750\n",
            "\u001b[32m2021-07-29T14:10:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:10:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:10:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:10:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:10:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:10:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.5390, train/hateful_memes/cross_entropy/avg: 0.5612, train/total_loss: 0.5390, train/total_loss/avg: 0.5612, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00002, ups: 1.06, time: 01m 34s 076ms, time_since_start: 13m 29s 668ms, eta: 37m 37s 834ms\n",
            "\u001b[32m2021-07-29T14:10:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:11:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:11:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:11:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:11:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:11:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/hateful_memes/cross_entropy: 0.8084, val/total_loss: 0.8084, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.2652, val/hateful_memes/roc_auc: 0.6562, num_updates: 600, epoch: 2, iterations: 600, max_updates: 3000, val_time: 49s 588ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.656250\n",
            "\u001b[32m2021-07-29T14:12:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:12:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:12:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:12:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:12:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:13:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.5390, train/hateful_memes/cross_entropy/avg: 0.5363, train/total_loss: 0.5390, train/total_loss/avg: 0.5363, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00002, ups: 1.02, time: 01m 38s 019ms, time_since_start: 15m 57s 278ms, eta: 37m 34s 444ms\n",
            "\u001b[32m2021-07-29T14:13:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:13:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:13:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:13:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:13:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/hateful_memes/cross_entropy: 0.7085, val/total_loss: 0.7085, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.5041, val/hateful_memes/roc_auc: 0.7034, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 48s 599ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.703397\n",
            "\u001b[32m2021-07-29T14:15:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:15:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.4525, train/hateful_memes/cross_entropy/avg: 0.5069, train/total_loss: 0.4525, train/total_loss/avg: 0.5069, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00002, ups: 1.06, time: 01m 34s 331ms, time_since_start: 18m 20s 211ms, eta: 34m 35s 291ms\n",
            "\u001b[32m2021-07-29T14:15:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:16:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/hateful_memes/cross_entropy: 0.8549, val/total_loss: 0.8549, val/hateful_memes/accuracy: 0.6444, val/hateful_memes/binary_f1: 0.3469, val/hateful_memes/roc_auc: 0.6961, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 33s 020ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.703397\n",
            "\u001b[32m2021-07-29T14:17:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:17:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:17:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:17:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:17:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:17:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.5390, train/hateful_memes/cross_entropy/avg: 0.5107, train/total_loss: 0.5390, train/total_loss/avg: 0.5107, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00002, ups: 1.03, time: 01m 37s 068ms, time_since_start: 20m 30s 303ms, eta: 33m 58s 442ms\n",
            "\u001b[32m2021-07-29T14:17:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:18:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:18:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:18:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:18:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:18:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/hateful_memes/cross_entropy: 0.8190, val/total_loss: 0.8190, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.2530, val/hateful_memes/roc_auc: 0.6693, num_updates: 900, epoch: 3, iterations: 900, max_updates: 3000, val_time: 31s 534ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.703397\n",
            "\u001b[32m2021-07-29T14:19:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:19:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:19:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:19:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:19:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:19:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.4575, train/hateful_memes/cross_entropy/avg: 0.5053, train/total_loss: 0.4575, train/total_loss/avg: 0.5053, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00003, ups: 1.04, time: 01m 36s 476ms, time_since_start: 22m 38s 316ms, eta: 32m 09s 528ms\n",
            "\u001b[32m2021-07-29T14:19:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:20:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.7881, val/total_loss: 0.7881, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3909, val/hateful_memes/roc_auc: 0.6926, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 32s 791ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.703397\n",
            "\u001b[32m2021-07-29T14:21:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:21:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:21:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:21:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:21:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:22:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.4575, train/hateful_memes/cross_entropy/avg: 0.4727, train/total_loss: 0.4575, train/total_loss/avg: 0.4727, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00003, ups: 1.06, time: 01m 34s 071ms, time_since_start: 24m 45s 182ms, eta: 29m 47s 355ms\n",
            "\u001b[32m2021-07-29T14:22:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:22:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/hateful_memes/cross_entropy: 0.9159, val/total_loss: 0.9159, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.3597, val/hateful_memes/roc_auc: 0.7245, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 49s 277ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.724515\n",
            "\u001b[32m2021-07-29T14:24:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:24:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.4525, train/hateful_memes/cross_entropy/avg: 0.4644, train/total_loss: 0.4525, train/total_loss/avg: 0.4644, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00003, ups: 1.08, time: 01m 33s 593ms, time_since_start: 27m 08s 055ms, eta: 28m 04s 688ms\n",
            "\u001b[32m2021-07-29T14:24:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:24:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:25:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/hateful_memes/cross_entropy: 0.7181, val/total_loss: 0.7181, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4257, val/hateful_memes/roc_auc: 0.6959, num_updates: 1200, epoch: 4, iterations: 1200, max_updates: 3000, val_time: 33s 683ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.724515\n",
            "\u001b[32m2021-07-29T14:26:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:26:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.4525, train/hateful_memes/cross_entropy/avg: 0.4436, train/total_loss: 0.4525, train/total_loss/avg: 0.4436, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00003, ups: 1.04, time: 01m 36s 041ms, time_since_start: 29m 17s 782ms, eta: 27m 12s 706ms\n",
            "\u001b[32m2021-07-29T14:26:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:26:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:27:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/hateful_memes/cross_entropy: 0.7480, val/total_loss: 0.7480, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4805, val/hateful_memes/roc_auc: 0.7162, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 32s 523ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.724515\n",
            "\u001b[32m2021-07-29T14:28:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:28:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.3872, train/hateful_memes/cross_entropy/avg: 0.4220, train/total_loss: 0.3872, train/total_loss/avg: 0.4220, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00003, ups: 1.08, time: 01m 33s 963ms, time_since_start: 31m 24s 275ms, eta: 25m 03s 416ms\n",
            "\u001b[32m2021-07-29T14:28:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:28:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:29:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/hateful_memes/cross_entropy: 0.7854, val/total_loss: 0.7854, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5322, val/hateful_memes/roc_auc: 0.7383, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 49s 716ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:30:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.3872, train/hateful_memes/cross_entropy/avg: 0.4035, train/total_loss: 0.3872, train/total_loss/avg: 0.4035, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00004, ups: 1.08, time: 01m 33s 507ms, time_since_start: 33m 47s 501ms, eta: 23m 22s 606ms\n",
            "\u001b[32m2021-07-29T14:31:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:31:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.7728, val/total_loss: 0.7728, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7246, num_updates: 1500, epoch: 5, iterations: 1500, max_updates: 3000, val_time: 33s 212ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:32:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:32:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:32:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:32:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:32:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:33:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.3722, train/hateful_memes/cross_entropy/avg: 0.3872, train/total_loss: 0.3722, train/total_loss/avg: 0.3872, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00004, ups: 1.04, time: 01m 36s 412ms, time_since_start: 35m 57s 128ms, eta: 22m 29s 776ms\n",
            "\u001b[32m2021-07-29T14:33:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:33:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:33:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:33:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:33:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:33:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/hateful_memes/cross_entropy: 1.5009, val/total_loss: 1.5009, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.3270, val/hateful_memes/roc_auc: 0.7007, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 33s 119ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:35:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:35:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.3872, train/hateful_memes/cross_entropy/avg: 0.3911, train/total_loss: 0.3872, train/total_loss/avg: 0.3911, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00004, ups: 1.06, time: 01m 34s 046ms, time_since_start: 38m 04s 297ms, eta: 20m 22s 599ms\n",
            "\u001b[32m2021-07-29T14:35:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:35:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:35:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, val/hateful_memes/cross_entropy: 0.9047, val/total_loss: 0.9047, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5245, val/hateful_memes/roc_auc: 0.7268, num_updates: 1700, epoch: 6, iterations: 1700, max_updates: 3000, val_time: 34s 287ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:37:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:37:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.3872, train/hateful_memes/cross_entropy/avg: 0.3922, train/total_loss: 0.3872, train/total_loss/avg: 0.3922, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00005, ups: 1.08, time: 01m 33s 337ms, time_since_start: 40m 11s 924ms, eta: 18m 40s 056ms\n",
            "\u001b[32m2021-07-29T14:37:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:38:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, val/hateful_memes/cross_entropy: 0.9281, val/total_loss: 0.9281, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5859, val/hateful_memes/roc_auc: 0.7085, num_updates: 1800, epoch: 6, iterations: 1800, max_updates: 3000, val_time: 32s 650ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:39:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:39:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.3872, train/hateful_memes/cross_entropy/avg: 0.3722, train/total_loss: 0.3872, train/total_loss/avg: 0.3722, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00005, ups: 1.05, time: 01m 35s 278ms, time_since_start: 42m 19s 857ms, eta: 17m 28s 060ms\n",
            "\u001b[32m2021-07-29T14:39:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:39:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:40:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, val/hateful_memes/cross_entropy: 1.0430, val/total_loss: 1.0430, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.5664, val/hateful_memes/roc_auc: 0.7295, num_updates: 1900, epoch: 7, iterations: 1900, max_updates: 3000, val_time: 33s 677ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:41:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:41:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.3722, train/hateful_memes/cross_entropy/avg: 0.3619, train/total_loss: 0.3722, train/total_loss/avg: 0.3619, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00005, ups: 1.06, time: 01m 34s 133ms, time_since_start: 44m 27s 670ms, eta: 15m 41s 338ms\n",
            "\u001b[32m2021-07-29T14:41:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:41:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:42:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 1.1885, val/total_loss: 1.1885, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4785, val/hateful_memes/roc_auc: 0.7035, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 3000, val_time: 33s 268ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:43:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:43:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:43:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:43:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:43:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:43:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.3652, train/hateful_memes/cross_entropy/avg: 0.3494, train/total_loss: 0.3652, train/total_loss/avg: 0.3494, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00005, ups: 1.08, time: 01m 33s 294ms, time_since_start: 46m 34s 236ms, eta: 13m 59s 650ms\n",
            "\u001b[32m2021-07-29T14:43:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:44:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, val/hateful_memes/cross_entropy: 1.1070, val/total_loss: 1.1070, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5260, val/hateful_memes/roc_auc: 0.7267, num_updates: 2100, epoch: 7, iterations: 2100, max_updates: 3000, val_time: 33s 661ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:45:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:45:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:45:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:45:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:45:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:46:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.3007, train/hateful_memes/cross_entropy/avg: 0.3349, train/total_loss: 0.3007, train/total_loss/avg: 0.3349, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00004, ups: 1.05, time: 01m 35s 439ms, time_since_start: 48m 43s 339ms, eta: 12m 43s 520ms\n",
            "\u001b[32m2021-07-29T14:46:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:46:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:46:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:46:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:46:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, val/hateful_memes/cross_entropy: 1.6310, val/total_loss: 1.6310, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4408, val/hateful_memes/roc_auc: 0.7097, num_updates: 2200, epoch: 8, iterations: 2200, max_updates: 3000, val_time: 32s 478ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:47:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:48:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.1951, train/hateful_memes/cross_entropy/avg: 0.3207, train/total_loss: 0.1951, train/total_loss/avg: 0.3207, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00003, ups: 1.06, time: 01m 34s 139ms, time_since_start: 50m 49s 960ms, eta: 10m 58s 978ms\n",
            "\u001b[32m2021-07-29T14:48:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:48:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:48:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:48:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:48:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:48:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/hateful_memes/cross_entropy: 1.4357, val/total_loss: 1.4357, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4337, val/hateful_memes/roc_auc: 0.6960, num_updates: 2300, epoch: 8, iterations: 2300, max_updates: 3000, val_time: 34s 432ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:49:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:50:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.1671, train/hateful_memes/cross_entropy/avg: 0.3143, train/total_loss: 0.1671, train/total_loss/avg: 0.3143, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00003, ups: 1.08, time: 01m 33s 786ms, time_since_start: 52m 58s 181ms, eta: 09m 22s 721ms\n",
            "\u001b[32m2021-07-29T14:50:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:50:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, val/hateful_memes/cross_entropy: 1.3243, val/total_loss: 1.3243, val/hateful_memes/accuracy: 0.7074, val/hateful_memes/binary_f1: 0.4936, val/hateful_memes/roc_auc: 0.7251, num_updates: 2400, epoch: 8, iterations: 2400, max_updates: 3000, val_time: 32s 549ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:52:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:52:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.1655, train/hateful_memes/cross_entropy/avg: 0.3019, train/total_loss: 0.1655, train/total_loss/avg: 0.3019, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00003, ups: 1.03, time: 01m 37s 589ms, time_since_start: 55m 08s 322ms, eta: 08m 07s 945ms\n",
            "\u001b[32m2021-07-29T14:52:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:52:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:53:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 1.3383, val/total_loss: 1.3383, val/hateful_memes/accuracy: 0.7111, val/hateful_memes/binary_f1: 0.5667, val/hateful_memes/roc_auc: 0.7256, num_updates: 2500, epoch: 9, iterations: 2500, max_updates: 3000, val_time: 34s 190ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:54:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:54:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.1467, train/hateful_memes/cross_entropy/avg: 0.2905, train/total_loss: 0.1467, train/total_loss/avg: 0.2905, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0.00002, ups: 1.08, time: 01m 33s 097ms, time_since_start: 57m 15s 614ms, eta: 06m 12s 390ms\n",
            "\u001b[32m2021-07-29T14:54:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:54:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:55:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, val/hateful_memes/cross_entropy: 1.5978, val/total_loss: 1.5978, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5289, val/hateful_memes/roc_auc: 0.7298, num_updates: 2600, epoch: 9, iterations: 2600, max_updates: 3000, val_time: 33s 661ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:56:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:56:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.1440, train/hateful_memes/cross_entropy/avg: 0.2799, train/total_loss: 0.1440, train/total_loss/avg: 0.2799, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00002, ups: 1.06, time: 01m 34s 665ms, time_since_start: 59m 23s 943ms, eta: 04m 43s 996ms\n",
            "\u001b[32m2021-07-29T14:56:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:57:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/hateful_memes/cross_entropy: 1.4628, val/total_loss: 1.4628, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5105, val/hateful_memes/roc_auc: 0.7194, num_updates: 2700, epoch: 9, iterations: 2700, max_updates: 3000, val_time: 34s 001ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T14:58:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:58:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.1431, train/hateful_memes/cross_entropy/avg: 0.2700, train/total_loss: 0.1431, train/total_loss/avg: 0.2700, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00001, ups: 1.04, time: 01m 36s 006ms, time_since_start: 01h 01m 33s 959ms, eta: 03m 12s 012ms\n",
            "\u001b[32m2021-07-29T14:58:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:59:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:59:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:59:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T14:59:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T14:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/hateful_memes/cross_entropy: 1.5422, val/total_loss: 1.5422, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7176, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 33s 139ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T15:00:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:00:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:00:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:00:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:00:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:01:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.1414, train/hateful_memes/cross_entropy/avg: 0.2624, train/total_loss: 0.1414, train/total_loss/avg: 0.2624, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00001, ups: 1.05, time: 01m 35s 684ms, time_since_start: 01h 03m 42s 791ms, eta: 01m 35s 684ms\n",
            "\u001b[32m2021-07-29T15:01:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:01:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:01:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:01:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:01:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/hateful_memes/cross_entropy: 1.6912, val/total_loss: 1.6912, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4923, val/hateful_memes/roc_auc: 0.7080, num_updates: 2900, epoch: 10, iterations: 2900, max_updates: 3000, val_time: 30s 369ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T15:02:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:02:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:02:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:02:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:02:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:03:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.1001, train/hateful_memes/cross_entropy/avg: 0.2537, train/total_loss: 0.1001, train/total_loss/avg: 0.2537, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 755ms, time_since_start: 01h 05m 46s 921ms, eta: 0ms\n",
            "\u001b[32m2021-07-29T15:03:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:03:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 1.6862, val/total_loss: 1.6862, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5075, val/hateful_memes/roc_auc: 0.7105, num_updates: 3000, epoch: 10, iterations: 3000, max_updates: 3000, val_time: 33s 324ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.738309\n",
            "\u001b[32m2021-07-29T15:03:41 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-07-29T15:03:41 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-07-29T15:03:41 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:04:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T15:04:00 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n",
            "\u001b[32m2021-07-29T15:04:00 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n",
            "\u001b[32m2021-07-29T15:04:00 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n",
            "\u001b[32m2021-07-29T15:04:02 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 18/18 [00:11<00:00,  1.60it/s]\n",
            "\u001b[32m2021-07-29T15:04:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/hateful_memes/cross_entropy: 0.7854, val/total_loss: 0.7854, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5322, val/hateful_memes/roc_auc: 0.7383\n",
            "\u001b[32m2021-07-29T15:04:13 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 06m 53s 137ms\n",
            "Deleting model files for space saving\n",
            "Current: 30_5e-06_0.1\n",
            "2021-07-29 15:04:21.717190: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/from_coco.yaml\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc.full\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 100\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.enabled to True\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.criteria to hateful_memes/roc_auc\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.minimize to False\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.max_features to 100\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.1\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 30\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-06\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/submissions/best/visual_bert/30_5e-06_0.1\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-06_0.1\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf: \u001b[0mLogging to: /content/submissions/best/visual_bert/30_5e-06_0.1/train.log\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.pretrained.cc.full', 'training.tensorboard=True', 'training.checkpoint_interval=100', 'training.evaluation_interval=100', 'training.max_updates=3000', 'training.log_interval=100', 'training.early_stop.enabled=True', 'training.early_stop.criteria=hateful_memes/roc_auc', 'training.early_stop.minimize=False', 'dataset_config.hateful_memes.max_features=100', 'dataset_config.hateful_memes.annotations.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.val[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.test[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'training.lr_ratio=0.1', 'training.use_warmup=True', 'training.batch_size=30', 'optimizer.params.lr=5e-06', 'env.save_dir=/content/submissions/best/visual_bert/30_5e-06_0.1', 'env.tensorboard_logdir=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-06_0.1'])\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf_cli.run: \u001b[0mUsing seed 28855843\n",
            "\u001b[32m2021-07-29T15:04:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2021-07-29T15:04:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2021-07-29T15:04:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-07-29T15:04:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2021-07-29T15:04:49 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:04:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2021-07-29T15:04:57 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2021-07-29T15:06:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:06:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.7118, train/hateful_memes/cross_entropy/avg: 0.7118, train/total_loss: 0.7118, train/total_loss/avg: 0.7118, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 1.02, time: 01m 38s 732ms, time_since_start: 01m 46s 728ms, eta: 47m 43s 249ms\n",
            "\u001b[32m2021-07-29T15:06:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:07:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/hateful_memes/cross_entropy: 0.6611, val/total_loss: 0.6611, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0196, val/hateful_memes/roc_auc: 0.5104, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 48s 712ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.510441\n",
            "\u001b[32m2021-07-29T15:08:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:08:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:08:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:08:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:08:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:08:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.7069, train/hateful_memes/cross_entropy/avg: 0.7094, train/total_loss: 0.7069, train/total_loss/avg: 0.7094, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 251ms, time_since_start: 04m 10s 308ms, eta: 43m 59s 041ms\n",
            "\u001b[32m2021-07-29T15:08:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:09:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:09:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:09:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:09:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:09:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/hateful_memes/cross_entropy: 0.6613, val/total_loss: 0.6613, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5167, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 49s 462ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.516750\n",
            "\u001b[32m2021-07-29T15:11:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:11:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.7069, train/hateful_memes/cross_entropy/avg: 0.6963, train/total_loss: 0.7069, train/total_loss/avg: 0.6963, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 703ms, time_since_start: 06m 34s 478ms, eta: 42m 37s 004ms\n",
            "\u001b[32m2021-07-29T15:11:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:11:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:12:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/hateful_memes/cross_entropy: 0.6618, val/total_loss: 0.6618, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.0099, val/hateful_memes/roc_auc: 0.5265, num_updates: 300, epoch: 1, iterations: 300, max_updates: 3000, val_time: 48s 931ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.526515\n",
            "\u001b[32m2021-07-29T15:13:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:13:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:13:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:13:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:13:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:13:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.6703, train/hateful_memes/cross_entropy/avg: 0.6658, train/total_loss: 0.6703, train/total_loss/avg: 0.6658, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 176ms, time_since_start: 08m 59s 589ms, eta: 41m 40s 589ms\n",
            "\u001b[32m2021-07-29T15:13:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:14:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:14:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:14:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:14:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:14:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/hateful_memes/cross_entropy: 0.6633, val/total_loss: 0.6633, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.0388, val/hateful_memes/roc_auc: 0.5371, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 49s 789ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.537103\n",
            "\u001b[32m2021-07-29T15:15:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:15:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:16:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6703, train/hateful_memes/cross_entropy/avg: 0.6660, train/total_loss: 0.6703, train/total_loss/avg: 0.6660, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 303ms, time_since_start: 11m 24s 685ms, eta: 39m 42s 585ms\n",
            "\u001b[32m2021-07-29T15:16:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:16:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:16:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:16:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:16:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:17:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.6623, val/total_loss: 0.6623, val/hateful_memes/accuracy: 0.6315, val/hateful_memes/binary_f1: 0.1603, val/hateful_memes/roc_auc: 0.5552, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 47s 848ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.555235\n",
            "\u001b[32m2021-07-29T15:18:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:18:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6490, train/total_loss: 0.6669, train/total_loss/avg: 0.6490, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 211ms, time_since_start: 13m 47s 748ms, eta: 38m 05s 075ms\n",
            "\u001b[32m2021-07-29T15:18:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:18:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:19:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/hateful_memes/cross_entropy: 0.6919, val/total_loss: 0.6919, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.1912, val/hateful_memes/roc_auc: 0.5765, num_updates: 600, epoch: 2, iterations: 600, max_updates: 3000, val_time: 48s 574ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.576529\n",
            "\u001b[32m2021-07-29T15:20:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:20:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:20:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:20:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:20:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:21:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6461, train/total_loss: 0.6669, train/total_loss/avg: 0.6461, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 135ms, time_since_start: 16m 12s 461ms, eta: 36m 51s 108ms\n",
            "\u001b[32m2021-07-29T15:21:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:21:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/hateful_memes/cross_entropy: 0.7365, val/total_loss: 0.7365, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.1570, val/hateful_memes/roc_auc: 0.5863, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 49s 608ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.586309\n",
            "\u001b[32m2021-07-29T15:23:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:23:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.6282, train/hateful_memes/cross_entropy/avg: 0.6436, train/total_loss: 0.6282, train/total_loss/avg: 0.6436, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 741ms, time_since_start: 18m 35s 813ms, eta: 34m 22s 307ms\n",
            "\u001b[32m2021-07-29T15:23:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:23:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:24:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/hateful_memes/cross_entropy: 0.7143, val/total_loss: 0.7143, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.2528, val/hateful_memes/roc_auc: 0.5984, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 49s 525ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.598397\n",
            "\u001b[32m2021-07-29T15:25:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:25:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:25:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:25:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:25:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:25:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.6282, train/hateful_memes/cross_entropy/avg: 0.6376, train/total_loss: 0.6282, train/total_loss/avg: 0.6376, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 956ms, time_since_start: 20m 59s 297ms, eta: 32m 53s 082ms\n",
            "\u001b[32m2021-07-29T15:25:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:26:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/hateful_memes/cross_entropy: 0.7025, val/total_loss: 0.7025, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.2846, val/hateful_memes/roc_auc: 0.6104, num_updates: 900, epoch: 3, iterations: 900, max_updates: 3000, val_time: 49s 494ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.610412\n",
            "\u001b[32m2021-07-29T15:27:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:27:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:27:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:27:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:27:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:28:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.6265, train/hateful_memes/cross_entropy/avg: 0.6240, train/total_loss: 0.6265, train/total_loss/avg: 0.6240, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 172ms, time_since_start: 23m 24s 966ms, eta: 32m 03s 460ms\n",
            "\u001b[32m2021-07-29T15:28:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:28:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:29:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.7415, val/total_loss: 0.7415, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.2528, val/hateful_memes/roc_auc: 0.6225, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 49s 465ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.622515\n",
            "\u001b[32m2021-07-29T15:30:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:30:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.6265, train/hateful_memes/cross_entropy/avg: 0.6209, train/total_loss: 0.6265, train/total_loss/avg: 0.6209, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 428ms, time_since_start: 25m 48s 864ms, eta: 29m 54s 148ms\n",
            "\u001b[32m2021-07-29T15:30:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:30:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:31:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/hateful_memes/cross_entropy: 0.7032, val/total_loss: 0.7032, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.4071, val/hateful_memes/roc_auc: 0.6307, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 48s 881ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.630721\n",
            "\u001b[32m2021-07-29T15:32:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:32:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:32:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:32:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:32:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.5900, train/hateful_memes/cross_entropy/avg: 0.6046, train/total_loss: 0.5900, train/total_loss/avg: 0.6046, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 761ms, time_since_start: 28m 11s 509ms, eta: 28m 07s 699ms\n",
            "\u001b[32m2021-07-29T15:33:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:33:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/hateful_memes/cross_entropy: 0.7256, val/total_loss: 0.7256, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.3298, val/hateful_memes/roc_auc: 0.6298, num_updates: 1200, epoch: 4, iterations: 1200, max_updates: 3000, val_time: 32s 818ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.630721\n",
            "\u001b[32m2021-07-29T15:34:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:34:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:34:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:34:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:34:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:35:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.5900, train/hateful_memes/cross_entropy/avg: 0.5814, train/total_loss: 0.5900, train/total_loss/avg: 0.5814, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 187ms, time_since_start: 30m 19s 517ms, eta: 26m 58s 187ms\n",
            "\u001b[32m2021-07-29T15:35:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:35:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:35:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:35:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:35:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:35:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/hateful_memes/cross_entropy: 0.8803, val/total_loss: 0.8803, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.2308, val/hateful_memes/roc_auc: 0.6338, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 50s 566ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.633824\n",
            "\u001b[32m2021-07-29T15:37:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:37:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.5900, train/hateful_memes/cross_entropy/avg: 0.5868, train/total_loss: 0.5900, train/total_loss/avg: 0.5868, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 089ms, time_since_start: 32m 44s 177ms, eta: 25m 05s 433ms\n",
            "\u001b[32m2021-07-29T15:37:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:38:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/hateful_memes/cross_entropy: 0.8088, val/total_loss: 0.8088, val/hateful_memes/accuracy: 0.6444, val/hateful_memes/binary_f1: 0.2993, val/hateful_memes/roc_auc: 0.6576, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 49s 023ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.657559\n",
            "\u001b[32m2021-07-29T15:39:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:39:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:39:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:39:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:39:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:39:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.5900, train/hateful_memes/cross_entropy/avg: 0.5847, train/total_loss: 0.5900, train/total_loss/avg: 0.5847, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 031ms, time_since_start: 35m 08s 234ms, eta: 23m 45s 468ms\n",
            "\u001b[32m2021-07-29T15:39:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:40:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.7751, val/total_loss: 0.7751, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.3004, val/hateful_memes/roc_auc: 0.6649, num_updates: 1500, epoch: 5, iterations: 1500, max_updates: 3000, val_time: 48s 030ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.664853\n",
            "\u001b[32m2021-07-29T15:42:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:42:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.5892, train/hateful_memes/cross_entropy/avg: 0.5720, train/total_loss: 0.5892, train/total_loss/avg: 0.5720, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 384ms, time_since_start: 37m 33s 652ms, eta: 22m 43s 381ms\n",
            "\u001b[32m2021-07-29T15:42:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:43:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/hateful_memes/cross_entropy: 0.7502, val/total_loss: 0.7502, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.4416, val/hateful_memes/roc_auc: 0.6660, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 48s 790ms, best_update: 1600, best_iteration: 1600, best_val/hateful_memes/roc_auc: 0.666044\n",
            "\u001b[32m2021-07-29T15:44:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:44:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.5892, train/hateful_memes/cross_entropy/avg: 0.5587, train/total_loss: 0.5892, train/total_loss/avg: 0.5587, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 419ms, time_since_start: 39m 56s 864ms, eta: 20m 27s 452ms\n",
            "\u001b[32m2021-07-29T15:44:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:45:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, val/hateful_memes/cross_entropy: 0.7397, val/total_loss: 0.7397, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4172, val/hateful_memes/roc_auc: 0.6797, num_updates: 1700, epoch: 6, iterations: 1700, max_updates: 3000, val_time: 49s 149ms, best_update: 1700, best_iteration: 1700, best_val/hateful_memes/roc_auc: 0.679721\n",
            "\u001b[32m2021-07-29T15:46:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:46:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:46:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:46:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:46:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:47:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.5742, train/hateful_memes/cross_entropy/avg: 0.5484, train/total_loss: 0.5742, train/total_loss/avg: 0.5484, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 262ms, time_since_start: 42m 20s 279ms, eta: 18m 51s 154ms\n",
            "\u001b[32m2021-07-29T15:47:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:47:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:47:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:47:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:47:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:48:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, val/hateful_memes/cross_entropy: 0.6921, val/total_loss: 0.6921, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.4685, val/hateful_memes/roc_auc: 0.6825, num_updates: 1800, epoch: 6, iterations: 1800, max_updates: 3000, val_time: 51s 166ms, best_update: 1800, best_iteration: 1800, best_val/hateful_memes/roc_auc: 0.682456\n",
            "\u001b[32m2021-07-29T15:49:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.5742, train/hateful_memes/cross_entropy/avg: 0.5399, train/total_loss: 0.5742, train/total_loss/avg: 0.5399, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 665ms, time_since_start: 44m 47s 114ms, eta: 17m 32s 322ms\n",
            "\u001b[32m2021-07-29T15:49:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:50:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, val/hateful_memes/cross_entropy: 0.8277, val/total_loss: 0.8277, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.3370, val/hateful_memes/roc_auc: 0.6895, num_updates: 1900, epoch: 7, iterations: 1900, max_updates: 3000, val_time: 51s 961ms, best_update: 1900, best_iteration: 1900, best_val/hateful_memes/roc_auc: 0.689500\n",
            "\u001b[32m2021-07-29T15:51:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:51:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:51:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:51:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:51:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:52:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.5641, train/hateful_memes/cross_entropy/avg: 0.5373, train/total_loss: 0.5641, train/total_loss/avg: 0.5373, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00001, ups: 1.06, time: 01m 34s 072ms, time_since_start: 47m 13s 151ms, eta: 15m 40s 725ms\n",
            "\u001b[32m2021-07-29T15:52:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:52:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 0.8180, val/total_loss: 0.8180, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4365, val/hateful_memes/roc_auc: 0.6883, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 3000, val_time: 32s 858ms, best_update: 1900, best_iteration: 1900, best_val/hateful_memes/roc_auc: 0.689500\n",
            "\u001b[32m2021-07-29T15:53:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:53:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:53:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:53:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:53:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:54:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.5546, train/hateful_memes/cross_entropy/avg: 0.5240, train/total_loss: 0.5546, train/total_loss/avg: 0.5240, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 554ms, time_since_start: 49m 19s 567ms, eta: 14m 01s 994ms\n",
            "\u001b[32m2021-07-29T15:54:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:54:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, val/hateful_memes/cross_entropy: 0.7785, val/total_loss: 0.7785, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4667, val/hateful_memes/roc_auc: 0.6957, num_updates: 2100, epoch: 7, iterations: 2100, max_updates: 3000, val_time: 50s 949ms, best_update: 2100, best_iteration: 2100, best_val/hateful_memes/roc_auc: 0.695750\n",
            "\u001b[32m2021-07-29T15:56:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:56:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.5019, train/hateful_memes/cross_entropy/avg: 0.5162, train/total_loss: 0.5019, train/total_loss/avg: 0.5162, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 411ms, time_since_start: 51m 46s 932ms, eta: 12m 51s 295ms\n",
            "\u001b[32m2021-07-29T15:56:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:56:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:57:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, val/hateful_memes/cross_entropy: 0.8540, val/total_loss: 0.8540, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4304, val/hateful_memes/roc_auc: 0.6942, num_updates: 2200, epoch: 8, iterations: 2200, max_updates: 3000, val_time: 32s 923ms, best_update: 2100, best_iteration: 2100, best_val/hateful_memes/roc_auc: 0.695750\n",
            "\u001b[32m2021-07-29T15:58:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:58:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.4886, train/hateful_memes/cross_entropy/avg: 0.5038, train/total_loss: 0.4886, train/total_loss/avg: 0.5038, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 759ms, time_since_start: 53m 54s 618ms, eta: 11m 03s 316ms\n",
            "\u001b[32m2021-07-29T15:58:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T15:58:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T15:59:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/hateful_memes/cross_entropy: 0.9053, val/total_loss: 0.9053, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4041, val/hateful_memes/roc_auc: 0.7056, num_updates: 2300, epoch: 8, iterations: 2300, max_updates: 3000, val_time: 49s 905ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:00:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:00:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:00:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:00:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:00:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:01:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.4250, train/hateful_memes/cross_entropy/avg: 0.4972, train/total_loss: 0.4250, train/total_loss/avg: 0.4972, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 846ms, time_since_start: 56m 18s 372ms, eta: 09m 23s 076ms\n",
            "\u001b[32m2021-07-29T16:01:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:01:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:01:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:01:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:01:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:01:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, val/hateful_memes/cross_entropy: 0.9988, val/total_loss: 0.9988, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.3732, val/hateful_memes/roc_auc: 0.6928, num_updates: 2400, epoch: 8, iterations: 2400, max_updates: 3000, val_time: 32s 641ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:02:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:02:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:02:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:02:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:02:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:03:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.3877, train/hateful_memes/cross_entropy/avg: 0.4919, train/total_loss: 0.3877, train/total_loss/avg: 0.4919, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 342ms, time_since_start: 58m 26s 357ms, eta: 07m 56s 712ms\n",
            "\u001b[32m2021-07-29T16:03:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:03:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:03:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:03:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:03:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:03:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 0.9764, val/total_loss: 0.9764, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4516, val/hateful_memes/roc_auc: 0.7025, num_updates: 2500, epoch: 9, iterations: 2500, max_updates: 3000, val_time: 34s 059ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:05:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:05:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.3818, train/hateful_memes/cross_entropy/avg: 0.4793, train/total_loss: 0.3818, train/total_loss/avg: 0.4793, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 991ms, time_since_start: 01h 34s 413ms, eta: 06m 15s 964ms\n",
            "\u001b[32m2021-07-29T16:05:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:05:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:05:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, val/hateful_memes/cross_entropy: 0.9629, val/total_loss: 0.9629, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4586, val/hateful_memes/roc_auc: 0.7028, num_updates: 2600, epoch: 9, iterations: 2600, max_updates: 3000, val_time: 32s 061ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:07:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:07:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.3793, train/hateful_memes/cross_entropy/avg: 0.4756, train/total_loss: 0.3793, train/total_loss/avg: 0.4756, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0., ups: 1.09, time: 01m 32s 764ms, time_since_start: 01h 02m 39s 240ms, eta: 04m 38s 292ms\n",
            "\u001b[32m2021-07-29T16:07:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:07:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:08:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/hateful_memes/cross_entropy: 0.9924, val/total_loss: 0.9924, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4314, val/hateful_memes/roc_auc: 0.6981, num_updates: 2700, epoch: 9, iterations: 2700, max_updates: 3000, val_time: 34s 167ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:09:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:09:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.3721, train/hateful_memes/cross_entropy/avg: 0.4644, train/total_loss: 0.3721, train/total_loss/avg: 0.4644, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 964ms, time_since_start: 01h 04m 49s 375ms, eta: 03m 11s 929ms\n",
            "\u001b[32m2021-07-29T16:09:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/hateful_memes/cross_entropy: 1.0106, val/total_loss: 1.0106, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4379, val/hateful_memes/roc_auc: 0.7033, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 32s 936ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:11:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:11:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.3642, train/hateful_memes/cross_entropy/avg: 0.4516, train/total_loss: 0.3642, train/total_loss/avg: 0.4516, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 400ms, time_since_start: 01h 06m 59s 715ms, eta: 01m 37s 400ms\n",
            "\u001b[32m2021-07-29T16:11:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:12:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:12:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:12:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:12:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:12:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/hateful_memes/cross_entropy: 1.0458, val/total_loss: 1.0458, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4665, val/hateful_memes/roc_auc: 0.7031, num_updates: 2900, epoch: 10, iterations: 2900, max_updates: 3000, val_time: 30s 712ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:13:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:13:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:13:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:13:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:13:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:13:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/hateful_memes/cross_entropy: 0.3534, train/hateful_memes/cross_entropy/avg: 0.4420, train/total_loss: 0.3534, train/total_loss/avg: 0.4420, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 980ms, time_since_start: 01h 09m 06s 410ms, eta: 0ms\n",
            "\u001b[32m2021-07-29T16:13:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:14:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/hateful_memes/cross_entropy: 1.0295, val/total_loss: 1.0295, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4735, val/hateful_memes/roc_auc: 0.7042, num_updates: 3000, epoch: 10, iterations: 3000, max_updates: 3000, val_time: 33s 409ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.705632\n",
            "\u001b[32m2021-07-29T16:14:29 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-07-29T16:14:29 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-07-29T16:14:29 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:14:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:14:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T16:14:48 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2300\n",
            "\u001b[32m2021-07-29T16:14:48 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2300\n",
            "\u001b[32m2021-07-29T16:14:48 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n",
            "\u001b[32m2021-07-29T16:14:50 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 18/18 [00:11<00:00,  1.55it/s]\n",
            "\u001b[32m2021-07-29T16:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/hateful_memes/cross_entropy: 0.9053, val/total_loss: 0.9053, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4041, val/hateful_memes/roc_auc: 0.7056\n",
            "\u001b[32m2021-07-29T16:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 10m 12s 654ms\n",
            "Deleting model files for space saving\n",
            "Current: 30_5e-06_0.3\n",
            "2021-07-29 16:15:10.408214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/from_coco.yaml\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc.full\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 100\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.enabled to True\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.criteria to hateful_memes/roc_auc\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.early_stop.minimize to False\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.max_features to 100\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 30\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-06\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/submissions/best/visual_bert/30_5e-06_0.3\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to /content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-06_0.3\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf: \u001b[0mLogging to: /content/submissions/best/visual_bert/30_5e-06_0.3/train.log\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.pretrained.cc.full', 'training.tensorboard=True', 'training.checkpoint_interval=100', 'training.evaluation_interval=100', 'training.max_updates=3000', 'training.log_interval=100', 'training.early_stop.enabled=True', 'training.early_stop.criteria=hateful_memes/roc_auc', 'training.early_stop.minimize=False', 'dataset_config.hateful_memes.max_features=100', 'dataset_config.hateful_memes.annotations.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/train_miso.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.val[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'dataset_config.hateful_memes.features.test[0]=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/misogynistic_meme_features/features/content/features', 'training.lr_ratio=0.3', 'training.use_warmup=True', 'training.batch_size=30', 'optimizer.params.lr=5e-06', 'env.save_dir=/content/submissions/best/visual_bert/30_5e-06_0.3', 'env.tensorboard_logdir=/content/drive/MyDrive/Colab_Notebooks/hateful_memes/logs/best/visual_bert/30_5e-06_0.3'])\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf_cli.run: \u001b[0mUsing seed 17575431\n",
            "\u001b[32m2021-07-29T16:15:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2021-07-29T16:15:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2021-07-29T16:15:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-07-29T16:15:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2021-07-29T16:15:38 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:15:45 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:15:45 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:15:45 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:15:45 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-07-29T16:15:45 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-07-29T16:15:46 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2021-07-29T16:15:46 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2021-07-29T16:17:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:17:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/hateful_memes/cross_entropy: 0.9194, train/hateful_memes/cross_entropy/avg: 0.9194, train/total_loss: 0.9194, train/total_loss/avg: 0.9194, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 311ms, time_since_start: 01m 45s 382ms, eta: 47m 02s 039ms\n",
            "\u001b[32m2021-07-29T16:17:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:17:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:18:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/hateful_memes/cross_entropy: 0.8365, val/total_loss: 0.8365, val/hateful_memes/accuracy: 0.3704, val/hateful_memes/binary_f1: 0.5405, val/hateful_memes/roc_auc: 0.5275, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 49s 491ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.527485\n",
            "\u001b[32m2021-07-29T16:19:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:19:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/hateful_memes/cross_entropy: 0.5634, train/hateful_memes/cross_entropy/avg: 0.7414, train/total_loss: 0.5634, train/total_loss/avg: 0.7414, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 987ms, time_since_start: 04m 09s 872ms, eta: 44m 19s 644ms\n",
            "\u001b[32m2021-07-29T16:19:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:19:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:20:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/hateful_memes/cross_entropy: 0.6615, val/total_loss: 0.6615, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5269, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 33s 629ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.527485\n",
            "\u001b[32m2021-07-29T16:21:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:21:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/hateful_memes/cross_entropy: 0.6289, train/hateful_memes/cross_entropy/avg: 0.7039, train/total_loss: 0.6289, train/total_loss/avg: 0.7039, max mem: 8674.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 517ms, time_since_start: 06m 18s 021ms, eta: 42m 31s 982ms\n",
            "\u001b[32m2021-07-29T16:21:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:22:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:22:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:22:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:22:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:22:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/hateful_memes/cross_entropy: 0.6587, val/total_loss: 0.6587, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5537, num_updates: 300, epoch: 1, iterations: 300, max_updates: 3000, val_time: 49s 778ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.553735\n",
            "\u001b[32m2021-07-29T16:24:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:24:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/hateful_memes/cross_entropy: 0.6289, train/hateful_memes/cross_entropy/avg: 0.6986, train/total_loss: 0.6289, train/total_loss/avg: 0.6986, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 894ms, time_since_start: 08m 44s 697ms, eta: 41m 59s 265ms\n",
            "\u001b[32m2021-07-29T16:24:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:24:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:25:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/hateful_memes/cross_entropy: 0.6567, val/total_loss: 0.6567, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5644, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 49s 763ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.564382\n",
            "\u001b[32m2021-07-29T16:26:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:26:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/hateful_memes/cross_entropy: 0.6826, train/hateful_memes/cross_entropy/avg: 0.7011, train/total_loss: 0.6826, train/total_loss/avg: 0.7011, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 915ms, time_since_start: 11m 09s 379ms, eta: 39m 32s 888ms\n",
            "\u001b[32m2021-07-29T16:26:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:26:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:27:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/hateful_memes/cross_entropy: 0.6690, val/total_loss: 0.6690, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5742, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 49s 298ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.574235\n",
            "\u001b[32m2021-07-29T16:28:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:28:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:28:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:28:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:28:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:29:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/hateful_memes/cross_entropy: 0.6471, train/hateful_memes/cross_entropy/avg: 0.6921, train/total_loss: 0.6471, train/total_loss/avg: 0.6921, max mem: 8674.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 548ms, time_since_start: 13m 32s 229ms, eta: 37m 25s 156ms\n",
            "\u001b[32m2021-07-29T16:29:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:29:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:29:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:29:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:29:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:30:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/hateful_memes/cross_entropy: 0.6704, val/total_loss: 0.6704, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.0978, val/hateful_memes/roc_auc: 0.5791, num_updates: 600, epoch: 2, iterations: 600, max_updates: 3000, val_time: 49s 996ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.579074\n",
            "\u001b[32m2021-07-29T16:31:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:31:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/hateful_memes/cross_entropy: 0.6826, train/hateful_memes/cross_entropy/avg: 0.6941, train/total_loss: 0.6826, train/total_loss/avg: 0.6941, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 906ms, time_since_start: 16m 134ms, eta: 37m 31s 853ms\n",
            "\u001b[32m2021-07-29T16:31:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:31:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:32:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/hateful_memes/cross_entropy: 0.7068, val/total_loss: 0.7068, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0909, val/hateful_memes/roc_auc: 0.5861, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 50s 151ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.586147\n",
            "\u001b[32m2021-07-29T16:33:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:33:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:33:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:33:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:33:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:34:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/hateful_memes/cross_entropy: 0.6471, train/hateful_memes/cross_entropy/avg: 0.6795, train/total_loss: 0.6471, train/total_loss/avg: 0.6795, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 218ms, time_since_start: 18m 25s 508ms, eta: 34m 54s 811ms\n",
            "\u001b[32m2021-07-29T16:34:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:34:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:34:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:34:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:34:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:34:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/hateful_memes/cross_entropy: 0.7146, val/total_loss: 0.7146, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.2867, val/hateful_memes/roc_auc: 0.5905, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 47s 967ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.590529\n",
            "\u001b[32m2021-07-29T16:36:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:36:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/hateful_memes/cross_entropy: 0.6826, train/hateful_memes/cross_entropy/avg: 0.6875, train/total_loss: 0.6826, train/total_loss/avg: 0.6875, max mem: 8674.0, experiment: run, epoch: 3, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 707ms, time_since_start: 20m 47s 189ms, eta: 32m 47s 862ms\n",
            "\u001b[32m2021-07-29T16:36:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:37:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/hateful_memes/cross_entropy: 0.7819, val/total_loss: 0.7819, val/hateful_memes/accuracy: 0.6315, val/hateful_memes/binary_f1: 0.0995, val/hateful_memes/roc_auc: 0.5908, num_updates: 900, epoch: 3, iterations: 900, max_updates: 3000, val_time: 49s 741ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.590779\n",
            "\u001b[32m2021-07-29T16:38:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:38:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:38:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:38:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:38:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:38:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/hateful_memes/cross_entropy: 0.6471, train/hateful_memes/cross_entropy/avg: 0.6689, train/total_loss: 0.6471, train/total_loss/avg: 0.6689, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 423ms, time_since_start: 23m 13s 359ms, eta: 32m 08s 477ms\n",
            "\u001b[32m2021-07-29T16:38:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:39:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:39:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:39:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:39:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:39:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/hateful_memes/cross_entropy: 0.6913, val/total_loss: 0.6913, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3699, val/hateful_memes/roc_auc: 0.6200, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 49s 385ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.620029\n",
            "\u001b[32m2021-07-29T16:40:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:40:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:40:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:40:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:40:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:41:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/hateful_memes/cross_entropy: 0.6471, train/hateful_memes/cross_entropy/avg: 0.6648, train/total_loss: 0.6471, train/total_loss/avg: 0.6648, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 447ms, time_since_start: 25m 37s 195ms, eta: 29m 54s 501ms\n",
            "\u001b[32m2021-07-29T16:41:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:41:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:41:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:41:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:41:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/hateful_memes/cross_entropy: 0.7368, val/total_loss: 0.7368, val/hateful_memes/accuracy: 0.6315, val/hateful_memes/binary_f1: 0.3162, val/hateful_memes/roc_auc: 0.6288, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 49s 078ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.628765\n",
            "\u001b[32m2021-07-29T16:43:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:43:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/hateful_memes/cross_entropy: 0.6289, train/hateful_memes/cross_entropy/avg: 0.6543, train/total_loss: 0.6289, train/total_loss/avg: 0.6543, max mem: 8674.0, experiment: run, epoch: 4, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 867ms, time_since_start: 28m 01s 143ms, eta: 28m 27s 616ms\n",
            "\u001b[32m2021-07-29T16:43:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:43:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:44:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/hateful_memes/cross_entropy: 0.7138, val/total_loss: 0.7138, val/hateful_memes/accuracy: 0.6370, val/hateful_memes/binary_f1: 0.4132, val/hateful_memes/roc_auc: 0.6325, num_updates: 1200, epoch: 4, iterations: 1200, max_updates: 3000, val_time: 48s 648ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.632471\n",
            "\u001b[32m2021-07-29T16:45:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:46:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/hateful_memes/cross_entropy: 0.6289, train/hateful_memes/cross_entropy/avg: 0.6411, train/total_loss: 0.6289, train/total_loss/avg: 0.6411, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0., ups: 1.01, time: 01m 39s 303ms, time_since_start: 30m 29s 097ms, eta: 28m 08s 152ms\n",
            "\u001b[32m2021-07-29T16:46:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:46:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:46:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:46:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:46:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:46:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/hateful_memes/cross_entropy: 0.7480, val/total_loss: 0.7480, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3578, val/hateful_memes/roc_auc: 0.6340, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 48s 525ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.634015\n",
            "\u001b[32m2021-07-29T16:48:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:48:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/hateful_memes/cross_entropy: 0.6242, train/hateful_memes/cross_entropy/avg: 0.6186, train/total_loss: 0.6242, train/total_loss/avg: 0.6186, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 622ms, time_since_start: 32m 52s 247ms, eta: 25m 13s 953ms\n",
            "\u001b[32m2021-07-29T16:48:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:48:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:49:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/hateful_memes/cross_entropy: 0.7144, val/total_loss: 0.7144, val/hateful_memes/accuracy: 0.6370, val/hateful_memes/binary_f1: 0.4645, val/hateful_memes/roc_auc: 0.6491, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 49s 261ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.649059\n",
            "\u001b[32m2021-07-29T16:50:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:50:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:50:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:50:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:50:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:50:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/hateful_memes/cross_entropy: 0.6242, train/hateful_memes/cross_entropy/avg: 0.6166, train/total_loss: 0.6242, train/total_loss/avg: 0.6166, max mem: 8674.0, experiment: run, epoch: 5, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 916ms, time_since_start: 35m 17s 429ms, eta: 23m 58s 754ms\n",
            "\u001b[32m2021-07-29T16:50:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:51:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:51:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:51:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:51:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:51:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/hateful_memes/cross_entropy: 0.7007, val/total_loss: 0.7007, val/hateful_memes/accuracy: 0.6444, val/hateful_memes/binary_f1: 0.3684, val/hateful_memes/roc_auc: 0.6601, num_updates: 1500, epoch: 5, iterations: 1500, max_updates: 3000, val_time: 48s 584ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.660132\n",
            "\u001b[32m2021-07-29T16:53:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:53:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/hateful_memes/cross_entropy: 0.5883, train/hateful_memes/cross_entropy/avg: 0.5983, train/total_loss: 0.5883, train/total_loss/avg: 0.5983, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 963ms, time_since_start: 37m 41s 980ms, eta: 22m 23s 490ms\n",
            "\u001b[32m2021-07-29T16:53:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:53:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:54:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/hateful_memes/cross_entropy: 0.7660, val/total_loss: 0.7660, val/hateful_memes/accuracy: 0.6481, val/hateful_memes/binary_f1: 0.4062, val/hateful_memes/roc_auc: 0.6709, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 49s 423ms, best_update: 1600, best_iteration: 1600, best_val/hateful_memes/roc_auc: 0.670868\n",
            "\u001b[32m2021-07-29T16:55:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:55:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/hateful_memes/cross_entropy: 0.5883, train/hateful_memes/cross_entropy/avg: 0.5853, train/total_loss: 0.5883, train/total_loss/avg: 0.5853, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 215ms, time_since_start: 40m 05s 622ms, eta: 20m 24s 807ms\n",
            "\u001b[32m2021-07-29T16:55:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:56:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, val/hateful_memes/cross_entropy: 0.8707, val/total_loss: 0.8707, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.3121, val/hateful_memes/roc_auc: 0.6646, num_updates: 1700, epoch: 6, iterations: 1700, max_updates: 3000, val_time: 32s 730ms, best_update: 1600, best_iteration: 1600, best_val/hateful_memes/roc_auc: 0.670868\n",
            "\u001b[32m2021-07-29T16:57:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:57:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:57:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:57:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:57:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/hateful_memes/cross_entropy: 0.5775, train/hateful_memes/cross_entropy/avg: 0.5788, train/total_loss: 0.5775, train/total_loss/avg: 0.5788, max mem: 8674.0, experiment: run, epoch: 6, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 096ms, time_since_start: 42m 12s 450ms, eta: 18m 49s 153ms\n",
            "\u001b[32m2021-07-29T16:57:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:58:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:58:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:58:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:58:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T16:58:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, val/hateful_memes/cross_entropy: 0.8050, val/total_loss: 0.8050, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3392, val/hateful_memes/roc_auc: 0.6725, num_updates: 1800, epoch: 6, iterations: 1800, max_updates: 3000, val_time: 48s 747ms, best_update: 1800, best_iteration: 1800, best_val/hateful_memes/roc_auc: 0.672471\n",
            "\u001b[32m2021-07-29T16:59:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T16:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:00:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/hateful_memes/cross_entropy: 0.5775, train/hateful_memes/cross_entropy/avg: 0.5588, train/total_loss: 0.5775, train/total_loss/avg: 0.5588, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 389ms, time_since_start: 44m 36s 594ms, eta: 17m 29s 283ms\n",
            "\u001b[32m2021-07-29T17:00:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:00:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:00:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:00:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:00:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:01:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, val/hateful_memes/cross_entropy: 0.7307, val/total_loss: 0.7307, val/hateful_memes/accuracy: 0.6519, val/hateful_memes/binary_f1: 0.4863, val/hateful_memes/roc_auc: 0.6814, num_updates: 1900, epoch: 7, iterations: 1900, max_updates: 3000, val_time: 50s 504ms, best_update: 1900, best_iteration: 1900, best_val/hateful_memes/roc_auc: 0.681441\n",
            "\u001b[32m2021-07-29T17:02:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:02:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/hateful_memes/cross_entropy: 0.5634, train/hateful_memes/cross_entropy/avg: 0.5522, train/total_loss: 0.5634, train/total_loss/avg: 0.5522, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00001, ups: 1.08, time: 01m 33s 882ms, time_since_start: 47m 984ms, eta: 15m 38s 824ms\n",
            "\u001b[32m2021-07-29T17:02:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:02:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:03:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/hateful_memes/cross_entropy: 0.8853, val/total_loss: 0.8853, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.3383, val/hateful_memes/roc_auc: 0.6824, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 3000, val_time: 50s 669ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.682397\n",
            "\u001b[32m2021-07-29T17:04:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:04:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:04:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:04:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:04:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:05:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/hateful_memes/cross_entropy: 0.5390, train/hateful_memes/cross_entropy/avg: 0.5460, train/total_loss: 0.5390, train/total_loss/avg: 0.5460, max mem: 8674.0, experiment: run, epoch: 7, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 682ms, time_since_start: 49m 25s 337ms, eta: 14m 03s 138ms\n",
            "\u001b[32m2021-07-29T17:05:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:05:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:05:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:05:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:05:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:05:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, val/hateful_memes/cross_entropy: 0.7853, val/total_loss: 0.7853, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.4685, val/hateful_memes/roc_auc: 0.6905, num_updates: 2100, epoch: 7, iterations: 2100, max_updates: 3000, val_time: 49s 437ms, best_update: 2100, best_iteration: 2100, best_val/hateful_memes/roc_auc: 0.690515\n",
            "\u001b[32m2021-07-29T17:07:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:07:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/hateful_memes/cross_entropy: 0.5010, train/hateful_memes/cross_entropy/avg: 0.5316, train/total_loss: 0.5010, train/total_loss/avg: 0.5316, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0., ups: 1.03, time: 01m 37s 335ms, time_since_start: 51m 52s 113ms, eta: 12m 58s 683ms\n",
            "\u001b[32m2021-07-29T17:07:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:07:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:08:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, val/hateful_memes/cross_entropy: 0.8772, val/total_loss: 0.8772, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4611, val/hateful_memes/roc_auc: 0.6891, num_updates: 2200, epoch: 8, iterations: 2200, max_updates: 3000, val_time: 33s 018ms, best_update: 2100, best_iteration: 2100, best_val/hateful_memes/roc_auc: 0.690515\n",
            "\u001b[32m2021-07-29T17:09:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:09:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/hateful_memes/cross_entropy: 0.4827, train/hateful_memes/cross_entropy/avg: 0.5167, train/total_loss: 0.4827, train/total_loss/avg: 0.5167, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 879ms, time_since_start: 54m 01s 014ms, eta: 11m 11s 156ms\n",
            "\u001b[32m2021-07-29T17:09:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:10:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/hateful_memes/cross_entropy: 0.9212, val/total_loss: 0.9212, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4108, val/hateful_memes/roc_auc: 0.6988, num_updates: 2300, epoch: 8, iterations: 2300, max_updates: 3000, val_time: 48s 950ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.698824\n",
            "\u001b[32m2021-07-29T17:11:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:11:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:11:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:11:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:11:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:12:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/hateful_memes/cross_entropy: 0.4694, train/hateful_memes/cross_entropy/avg: 0.5017, train/total_loss: 0.4694, train/total_loss/avg: 0.5017, max mem: 8674.0, experiment: run, epoch: 8, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0., ups: 1.08, time: 01m 33s 097ms, time_since_start: 56m 23s 066ms, eta: 09m 18s 583ms\n",
            "\u001b[32m2021-07-29T17:12:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:12:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:12:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:12:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:12:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:12:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, val/hateful_memes/cross_entropy: 0.8941, val/total_loss: 0.8941, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4459, val/hateful_memes/roc_auc: 0.6974, num_updates: 2400, epoch: 8, iterations: 2400, max_updates: 3000, val_time: 33s 899ms, best_update: 2300, best_iteration: 2300, best_val/hateful_memes/roc_auc: 0.698824\n",
            "\u001b[32m2021-07-29T17:13:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:14:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/hateful_memes/cross_entropy: 0.4274, train/hateful_memes/cross_entropy/avg: 0.4892, train/total_loss: 0.4274, train/total_loss/avg: 0.4892, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 379ms, time_since_start: 58m 32s 346ms, eta: 07m 56s 895ms\n",
            "\u001b[32m2021-07-29T17:14:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:14:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:14:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:14:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:14:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:14:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/hateful_memes/cross_entropy: 0.9444, val/total_loss: 0.9444, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4566, val/hateful_memes/roc_auc: 0.7021, num_updates: 2500, epoch: 9, iterations: 2500, max_updates: 3000, val_time: 49s 554ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.702088\n",
            "\u001b[32m2021-07-29T17:16:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:16:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/hateful_memes/cross_entropy: 0.4215, train/hateful_memes/cross_entropy/avg: 0.4793, train/total_loss: 0.4215, train/total_loss/avg: 0.4793, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 720ms, time_since_start: 01h 58s 624ms, eta: 06m 26s 882ms\n",
            "\u001b[32m2021-07-29T17:16:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:16:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, val/hateful_memes/cross_entropy: 0.9841, val/total_loss: 0.9841, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4571, val/hateful_memes/roc_auc: 0.7038, num_updates: 2600, epoch: 9, iterations: 2600, max_updates: 3000, val_time: 48s 649ms, best_update: 2600, best_iteration: 2600, best_val/hateful_memes/roc_auc: 0.703765\n",
            "\u001b[32m2021-07-29T17:18:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:19:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/hateful_memes/cross_entropy: 0.4183, train/hateful_memes/cross_entropy/avg: 0.4770, train/total_loss: 0.4183, train/total_loss/avg: 0.4770, max mem: 8674.0, experiment: run, epoch: 9, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0., ups: 1.05, time: 01m 35s 230ms, time_since_start: 01h 03m 22s 506ms, eta: 04m 45s 691ms\n",
            "\u001b[32m2021-07-29T17:19:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:19:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:19:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:19:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:19:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:19:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/hateful_memes/cross_entropy: 1.0285, val/total_loss: 1.0285, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4186, val/hateful_memes/roc_auc: 0.6969, num_updates: 2700, epoch: 9, iterations: 2700, max_updates: 3000, val_time: 33s 305ms, best_update: 2600, best_iteration: 2600, best_val/hateful_memes/roc_auc: 0.703765\n",
            "\u001b[32m2021-07-29T17:20:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:20:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:21:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/hateful_memes/cross_entropy: 0.3770, train/hateful_memes/cross_entropy/avg: 0.4634, train/total_loss: 0.3770, train/total_loss/avg: 0.4634, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0., ups: 1.04, time: 01m 36s 065ms, time_since_start: 01h 05m 31s 880ms, eta: 03m 12s 131ms\n",
            "\u001b[32m2021-07-29T17:21:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:21:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/hateful_memes/cross_entropy: 1.0170, val/total_loss: 1.0170, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4673, val/hateful_memes/roc_auc: 0.6984, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 33s 249ms, best_update: 2600, best_iteration: 2600, best_val/hateful_memes/roc_auc: 0.703765\n",
            "\u001b[32m2021-07-29T17:22:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:23:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/hateful_memes/cross_entropy: 0.3260, train/hateful_memes/cross_entropy/avg: 0.4519, train/total_loss: 0.3260, train/total_loss/avg: 0.4519, max mem: 8674.0, experiment: run, epoch: 10, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0., ups: 1.06, time: 01m 34s 231ms, time_since_start: 01h 07m 39s 364ms, eta: 01m 34s 232ms\n",
            "\u001b[32m2021-07-29T17:23:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:23:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:23:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:23:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:23:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2021-07-29T17:23:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/hateful_memes/cross_entropy: 1.0179, val/total_loss: 1.0179, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4773, val/hateful_memes/roc_auc: 0.7019, num_updates: 2900, epoch: 10, iterations: 2900, max_updates: 3000, val_time: 33s 480ms, best_update: 2600, best_iteration: 2600, best_val/hateful_memes/roc_auc: 0.703765\n",
            "\u001b[32m2021-07-29T17:25:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-07-29T17:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
